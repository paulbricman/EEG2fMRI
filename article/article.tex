\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{graphicx}

\title{EEG2fMRI: Cross-Modal Synthesis for Functional Neuroimaging}
\author{Paul Bricman}
\author{Jelmer Borst}
\affil{University of Groningen}
\date{July 2021}

\begin{document}

\maketitle

\section{Introduction}

Neural activity is central to investigating cognition. In cognitive neuroscience, it has been employed to study the neural correlates of numerous cognitive functions, such as perception, attention, and memory. In cognitive modelling, neural activity has been used to inform the development of a wide range of cognitive architectures. In clinical neuroscience, it has been employed for investigating, diagnosing, and ameliorating a wide range of neurological conditions. 

The increased relevance of neural activity in cognitive science prompted the development of a large number of functional neuroimaging techniques. These techniques can be effectively described in terms of spatial resolution, temporal resolution (sampling rate), and portability. The most widely used functional neuroimaging techniques are EEG and fMRI. EEG scanners consist of arrays of electrodes placed directly on the user's scalp. They are extremely portable and have high temporal resolution. However, such scanners can only detect neural activity after first being filtered by the skull, leading to a major decrease in spatial resolution. Conversely, fMRI is based on electromagnetic waves which easily penetrate the skull. Therefore, it has high spatial resolution, but is lacking in most other aspects. Compared to EEG, it is hundreds of times more expensive and thousands of times slower. Moreover, it is virtually unmovable by its user.

Given the lack of techniques which individually exhibit high overall performance, there have been numerous attempts to integrate their complementary qualities by using multiple techniques simultaneously. EEG-informed fMRI, fMRI-informed EEG, and neurogenerative modelling are all methods of integrating data from simultaneous EEG and fMRI recordings. However, these methods require extensive access to both neuroimaging techniques involved and are limited to niched applications.

Besides multi-modal functional neuroimaging, investigators have also proposed uni-modal synthesis as a method of augmenting neuroimaging techniques. This consists of conditionally synthetizing novel data based on previous data collected through the same modality. This method has been found to increase performance in downstream machine learning classifiers based on fMRI data.

In addition to uni-modal synthesis, investigators have also proposed cross-modal synthesis for augmenting neuroimaging techniques. This consists of reconstructing data collected through a target modality based on data collected through a distinct source modality, by exploiting the inherent relation between them. In a setting where both source and target modalities are available, this task might have limited utility. However, the value of cross-modal synthesis comes from settings where only one modality is available. In such settings, existing data can be used to approximate data collected through the unavailable modality. Unfortunately, this approach has largely been used for augmenting structural neuroimaging, with limited analogous efforts for functional neuroimaging.

Despite the major demand for high-performance functional neuroimaging and the success of cross-modal synthesis for structural neuroimaging, limited attention has been given to cross-modal synthesis for functional neuroimaging. The purpose of the current work is to explore the feasibility of this method. This comes as a natural extension to the growing collection of functional neuroimaging augmentation methods.

\section{Task}

The task of modelling the relation between two functional neuroimaging modalities can be addressed through at least three different approaches. This decision is mainly based on data availablility.

First, if simultaneous multi-modal recordings are available, supervised learning can be employed to model the mapping between the two domains based on paired samples. Unfortunately, the constraint of physical compatibility greatly restricts the pairs of modalities which can be investigated. For instance, MR-compatible EEG scanners have to be manufactured with special materials, in order to avoid physical injury during EEG-fMRI operation. Fortunately, the development of multi-modal methods of data integration has highlighted multiple candidate pairs of modalities for simultaneous recording.

Second, if simultaneous multi-modal recordings are not available, but separate recordings of similar neural states with different modalities are available (i.e. two separate recording sessions based on the same task but using different techniques), semi-supervised learning can be used to model the mapping through a task-related signal. This is analogous to the task of machine translation, which is often based on sample pairing through a cross-lingual signal.

Third, if the quantity of such recordings is limited, unsupervised learning can be used to learn the mapping by learning to align the latent spaces associated with the two modalities. This is analogous to machine translation where no paired multilingual corpora exist.

The following sections focus on supervised cross-modal synthesis, as the setup it requires is the most straight-forward.

The suitability of EEG-fMRI as a candidate pair for supervised cross-modal synthesis is supported by the established practices of multi-modal neuroimaging using EEG-fMRI. These practices are the result of physical compatibility and the presence of complementary qualities.

Not only are EEG and fMRI compatible and complementary, but their results are also strongly correlated. The correlation occurs because both techniques can be used to record the same neural activity, albeit in different ways.

\section{Data}

We used a dataset from a multi-modal neuroimaging study which involved visual and auditory oddball tasks. The reason we chose this dataset, among other multi-modal neuroimaging ones, was that visual and auditory areas might be easier for the model to distinguish compared to more distributed networks of regions. Based on this dataset, we derived individual samples with the following structure. The input part of a data point consists of a 30-second-long EEG recording across 34 electrodes at 1000Hz. This data resembles 34 parallel time series unfolding across 30 seconds. Each time series originates from a certain electrode placed in a certain place on the subject's scalp.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{eeg.png}
    \caption{An input sample consisting of 34 parallel EEG signals}
2\end{figure}

The output part of a data point consists of a three-dimensional tensor of shape 53x63x52 which depicts the 3D "picture" of the subject's brain obtained through fMRI at the time roughly corresponding to the end of the EEG recording.

For the input part of a data point, the recording length of 30 seconds has been chosen for the following reason. fMRI doesn't record neural activity per se, but blood oxygen levels, the BOLD response, which is a reasonable proxy for neural activity. However, the BOLD response isn't an immediate one-to-one reflection of neural activity, but the result of a convolution between neural activity and a response curve informed by the brain's physiology (the BOLD response curve). This is conceptually similar to how "eating" activity isn't instantly reflected in blood sugar levels, but reflects the result of a convolution informed by metabolism. In the case of fMRI, recordings at any given time are influenced by neural activity which occured roughly during the previous 30 seconds. That's why 30 seconds of EEG are paired with one fMRI scan obtained at the end of the EEG recording, forming a data point. In contrast, EEG data itself closely reflects neural activity unfolding at that exact time of recording (albeit with very poor spatial resolution).

The raw original dataset is structured as follows. There are 16 subjects (after excluding one whose data was corrupted), each undergoing 6 blocks of an oddball task. Each block contains 170 fMRI scans, obtained sequentially, at a frequency of 0.5 Hz (for a total of about ~340 seconds per block). Each block also contains EEG data, recorded constantly at 1000Hz for the entire duration of the block. From these longer sequences of data, we're extracting data points in the way described previously. However, we're discarding the first few fMRI scans obtained before 30 seconds of EEG managed to accumulate beforehand. This means that ~155 samples are derived per block, which translates to 930 per subject, which translates to 14880 in total.

In terms of preprocessing, EEG data was normalized per subject AND per channel. The reasoning behind this was to remove influences of electrodes being placed in slightly different locations across subjects. The preprocessing pipeline for fMRI is more complex, consisting of: motion correction, slice time correction, registration, normalization, and smoothing. Additionally, each tensor obtained through a fMRI scan has been "masked" in order to only contain non-zero values at locations which are known to contain white and gray matter. All these preprocessing stages where used to increase the signal-to-noise ration in both EEG and fMRI, limiting the influence of subject-specific particularities on the data so that the mapping would be performed as effectively as possible.

\end{document}
